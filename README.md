# Requirements and Design for Reinforcement Learning and Safety test bed.

## 1. Introduction
We are looking to test and implement a strategy for safety in machine learning agents. In this project we will build a test bed in which an agent has to achieve some goal G. The implementation of the project will support a machine learning agent in the form of reinforcement learning that will evaluate its environment, and produce optimal strategies to achieve its goal state from any possible initial state in itâ€™s environment. In order to test safety strategies for this agent we will introduce
danger situations into a non-deterministic environment and our algorithms/agent will have to work around these circumstances to obtain not only an optimal strategy but also a safe strategy. The project will be developed in Python.

## 2. Domain
The domain of this test bed will involve a planetary robot in a starting point (initial state sstart) that moves around in a grid world and whoâ€™s task is to reach a target destination (a goal state sgoal). Thegrid world will have craters in which the robot can fall (safety concern) and borders that limit theirmovement to a finite number of locations. The robot, or agent, will have the ability to observe thestate of its environment (sâˆˆS), and a set of actions (aâˆˆA) it can perform to alter this state. Also our environment will be non-deterministic, where an agent has 95% chance of performing the intended action and 5% chance of performing any other possible action. Here is a visual example of a grid world with a goal state (green square), crates (red squares) and and initial state (blue circle which is the robot):

The learning algorithm will not be limited to this domain. But for test purposes I will implement a reinforcement learning algorithm in the context of a grid world. The algorithm will be able to work on any circumstances as long as a set of states (including initial and goal state), actions, rewards and punishments are defined. I will be able to adapt the algorithm to any domain.

## 3. Architecture & Components of Test bed
The test bed will be divided in 3 main components, the environment, the intelligent agent and the q-learner. The environment will be a class that contains all the data necessary to describe and act on an environment, it includes a tuple of states, actions, a reward function and a transition function. The agent will be a class that contains all data necessary for an agent, it includes the specific environment the agent is acting upon, the current state of agent, a transition function, itâ€™s agent function and a safety Boolean function.

### 3.1. Environment Specification

The environment will be specified as a class (i.e. environment.py) with the following components:
â€¢ States, ğ‘†: set of states s.t every s âˆˆ ğ‘† is a possible state of the environment.
â€¢ Actions, ğ´: set of actions s.t every a âˆˆ ğ´ is an action that an agent can perform in this environment.
â€¢ Reward Function, ğ‘Ÿ: ğ‘†, ğ´ â†’ ğ‘… s.t for each pair of state/action inputs (ğ‘ , ğ‘) it will output the reward r âˆˆ â„ an agent would obtain by taking an action ğ‘ from state ğ‘  in this environment.
â€¢ Transition Function, ğ‘¡ğ‘“: ğ‘†, ğ´ â†’ ğ‘† s.t for each pair of state-action inputs (ğ‘ 3, ğ‘) it will output a state ğ‘ 4 an agent would transition into if the agent would take action ğ‘ from state ğ‘ 3 (i.e. if agent is in ğ‘ 3 = [0,1], and I take action ğ‘ = ğ‘ˆğ‘ƒ, then ğ‘¡ğ‘“(ğ‘ 3, ğ‘) = ğ‘ 4 = [0,2]).

### 3.2. Intelligent Agent Specification
The intelligent agent will be specified as a class (i.e. i-agent.py) with the following components,
to build a complete agent one needs at least a specific environment it will run on and a matrix that
will represent the agentâ€™s knowledge of the maximum potential reward for each possible stateaction
pair (in this case based on Q-learning):
â€¢ State, s: the current state the agent finds himself in. t
â€¢ Environment, environment class ğ‘’, such that ğ‘’ will be the specific environment the agent is acting on.
â€¢ Knowledge Base: matrix Q s.t. for each pair of state-actions, ğ‘„[ğ‘ ][ğ‘] is equivalent to the ğ‘„(ğ‘ , ğ‘) obtained from a Q-learning algorithm.
â€¢ Transition Function, ğ‘¡ğ‘“: ğ‘†, ğ´ â†’ ğ‘† s.t for each pair of state-action inputs (ğ‘ 3, ğ‘) it will output a new state ğ‘ 4 an agent would transition to if the agent would take action ğ‘ from state ğ‘ 3. (i.e. if agent is in ğ‘ 3 = [0,1], and I take action ğ‘ = ğ‘ˆğ‘ƒ, then ğ‘¡ğ‘“(ğ‘ 3, ğ‘) = ğ‘ 4 = [0,2])
â€¢ Agent Function, ğ‘¡ğ‘“: ğ‘† â†’ ğ´ s.t for each input state ğ‘ , it will output the action ğ‘ that provides the maximum reward.
â€¢ Safety Boolean Function, ğ‘ ğ‘: ğ‘†, ğ´ â†’ {ğ‘‡ğ‘Ÿğ‘¢ğ‘’, ğ¹ğ‘ğ‘™ğ‘ ğ‘’} s.t. for each pair of state/action inputs (ğ‘ , ğ‘) it will output ğ‘‡ğ‘Ÿğ‘¢ğ‘’ if the state obtained by applying ğ‘ to ğ‘  is safe and it will return ğ¹ğ‘ğ‘™ğ‘ ğ‘’ if it is not safe.

### 3.3 PEAS
These are the specific settings our agent will be working in:
â€¢ Performance measure: Safety of agent and ability to reach a goal from an initial state sstart
â€¢ Environment: Grid world with an initial state, a goal state and craters.
â€¢ Actuators: move forwards, backwards, to the right and to the left.
â€¢ Sensors: Ability of agent to inspect its current state.
