# Requirements and Design for Reinforcement Learning and Safety test bed.

## 1. Introduction
We are looking to test and implement a strategy for safety in machine learning agents. In this project we will build a test bed in which an agent has to achieve some goal G. The implementation of the project will support a machine learning agent in the form of reinforcement learning that will evaluate its environment, and produce optimal strategies to achieve its goal state from any possible initial state in itâ€™s environment. In order to test safety strategies for this agent we will introduce
danger situations into a non-deterministic environment and our algorithms/agent will have to work around these circumstances to obtain not only an optimal strategy but also a safe strategy. The project will be developed in Python.

## 2. Domain
The domain of this test bed will involve a planetary robot in a starting point (initial state sstart) that moves around in a grid world and whoâ€™s task is to reach a target destination (a goal state sgoal). Thegrid world will have craters in which the robot can fall (safety concern) and borders that limit theirmovement to a finite number of locations. The robot, or agent, will have the ability to observe thestate of its environment (sâˆˆS), and a set of actions (aâˆˆA) it can perform to alter this state. Also our environment will be non-deterministic, where an agent has 95% chance of performing the intended action and 5% chance of performing any other possible action. Here is a visual example of a grid world with a goal state (green square), crates (red squares) and and initial state (blue circle which is the robot):

The learning algorithm will not be limited to this domain. But for test purposes I will implement a reinforcement learning algorithm in the context of a grid world. The algorithm will be able to work on any circumstances as long as a set of states (including initial and goal state), actions, rewards and punishments are defined. I will be able to adapt the algorithm to any domain.

## 3. Architecture & Components of Test bed
The test bed will be divided in 3 main components, the environment, the intelligent agent and the q-learner. The environment will be a class that contains all the data necessary to describe and act on an environment, it includes a tuple of states, actions, a reward function and a transition function. The agent will be a class that contains all data necessary for an agent, it includes the specific environment the agent is acting upon, the current state of agent, a transition function, itâ€™s agent function and a safety Boolean function.

### 3.1. Environment Specification

The environment will be specified as a class (i.e. environment.py) with the following components:
- States, ğ‘†: set of states s.t every s âˆˆ ğ‘† is a possible state of the environment.
- Actions, ğ´: set of actions s.t every a âˆˆ ğ´ is an action that an agent can perform in this environment.
- Reward Function, ğ‘Ÿ: ğ‘†, ğ´ â†’ ğ‘… s.t for each pair of state/action inputs (ğ‘ , ğ‘) it will output the reward r âˆˆ â„ an agent would obtain by taking an action ğ‘ from state ğ‘  in this environment.
- Transition Function, ğ‘¡ğ‘“: ğ‘†, ğ´, p â†’ ğ‘† s.t for each pair of state-action inputs (ğ‘ 3, ğ‘) it will output a state ğ‘ 4 an agent would transition into if the agent would take action ğ‘ from state ğ‘ 3 (i.e. if agent is in ğ‘ 3 = [0,1], and I take action ğ‘ = ğ‘ˆğ‘ƒ, then ğ‘¡ğ‘“(ğ‘ 3, ğ‘) = ğ‘ 4 = [0,2]).

The environment will be observable, non-deterministic, static, discrete and single-agent.

### 3.2. Q-learner
The Q-learner will be specified as a class (i.e. Q-learning.py). The Q-learnner will be built based on a environment. Once we have the set sets, actions, the deterministc and the non-deterministic transition functions and the reward functions the Q-learner will generate a matrix Q:|S|x|A| with the Q(s,a) value for each state-action pair. The Q-learner will have the following components:
- Environment, env: class ğ‘’, such that ğ‘’ will be the specific environment the agent is acting on.
- Q-learning function: Will return the matrix Q:|S|x|A| s.t. for each pair of state functions s,a, q[s][a] will return the Q(s,a) value which is equivalent to the maximum cumulative reward that can be achieved by performing action ğ‘ on state ğ‘ 

### 3.3. Intelligent Agent Specification
The intelligent agent will be specified as a class (i.e. i-agent.py). To build a complete agent we need the environment it will run on and the Q-learning matrix generated by the Q-learner for the specific environment. The agent will have the following components.
- State, s: the current state the agent finds himself in. t
- Environment, env: class ğ‘’, such that ğ‘’ will be the specific environment the agent is acting on.
- Knowledge Base: matrix Q s.t. for each pair of state-actions, ğ‘„[ğ‘ ][ğ‘] is equivalent to the ğ‘„(ğ‘ , ğ‘) obtained from a Q-learning algorithm.
- Transition Function, ğ‘¡ğ‘“: ğ‘†, ğ´ â†’ ğ‘† s.t for each pair of state-action inputs (ğ‘ 3, ğ‘) it will output a new state ğ‘ 4 an agent would transition to if the agent would take action ğ‘ from state ğ‘ 3. (i.e. if agent is in ğ‘ 3 = [0,1], and I take action ğ‘ = ğ‘ˆğ‘ƒ, then ğ‘¡ğ‘“(ğ‘ 3, ğ‘) = ğ‘ 4 = [0,2])
- Agent Function, ğ‘¡ğ‘“: ğ‘† â†’ ğ´ s.t for each input state ğ‘ , it will output the action ğ‘ that provides the maximum reward.
- Safety Boolean Function, ğ‘ ğ‘: ğ‘†, ğ´ â†’ {ğ‘‡ğ‘Ÿğ‘¢ğ‘’, ğ¹ğ‘ğ‘™ğ‘ ğ‘’} s.t. for each pair of state/action inputs (ğ‘ , ğ‘) it will output ğ‘‡ğ‘Ÿğ‘¢ğ‘’ if the state obtained by applying ğ‘ to ğ‘  is safe and it will return ğ¹ğ‘ğ‘™ğ‘ ğ‘’ if it is not safe.

### 3.4. PEAS
These are the specific settings our agent will be working in:
- Performance measure: Safety of agent and ability to reach a goal from an initial state sstart
- Environment: Grid world with an initial state, a goal state and craters.
- Actuators: move forwards, backwards, to the right and to the left.
- Sensors: Ability of agent to inspect its current state.
