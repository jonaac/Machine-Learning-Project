# Requirements and Design for Reinforcement Learning and Safety test bed.

## 1. Introduction
We are looking to test and implement a strategy for safety in machine learning agents. In this project we will build a test bed in which an agent has to achieve some goal G. The implementation of the project will support a machine learning agent in the form of reinforcement learning that will evaluate its environment, and produce optimal strategies to achieve its goal state from any possible initial state in itâ€™s environment. In order to test safety strategies for this agent we will introduce
danger situations into a non-deterministic environment and our algorithms/agent will have to work around these circumstances to obtain not only an optimal strategy but also a safe strategy. The project will be developed in Python.

## 2. Domain
The domain of this test bed will involve a planetary robot in a starting point (initial state sstart) that moves around in a grid world and whoâ€™s task is to reach a target destination (a goal state sgoal). Thegrid world will have craters in which the robot can fall (safety concern) and borders that limit theirmovement to a finite number of locations. The robot, or agent, will have the ability to observe thestate of its environment (sâˆˆS), and a set of actions (aâˆˆA) it can perform to alter this state. Also our environment will be non-deterministic, where an agent has 95% chance of performing the intended action and 5% chance of performing any other possible action. Here is a visual example of a grid world with a goal state (green square), crates (red squares) and and initial state (blue circle which is the robot):

The learning algorithm will not be limited to this domain. But for test purposes I will implement a reinforcement learning algorithm in the context of a grid world. The algorithm will be able to work on any circumstances as long as a set of states (including initial and goal state), actions, rewards and punishments are defined. I will be able to adapt the algorithm to any domain.

## 3. Architecture & Components of Test bed
The test bed will be divided in 3 main components, the environment, the intelligent agent and the q-learner. The environment will be a class that contains all the data necessary to describe and act on an environment, it includes a tuple of states, actions, a reward function and a transition function. The agent will be a class that contains all data necessary for an agent, it includes the specific environment the agent is acting upon, the current state of agent, a transition function, itâ€™s agent function and a safety Boolean function.

### 3.1. Environment Specification

The environment will be specified as a class (i.e. environment.py) with the following components:
- States, ğ‘†: set of states s.t every s âˆˆ ğ‘† is a possible state of the environment.
- Actions, ğ´: set of actions s.t every a âˆˆ ğ´ is an action that an agent can perform in this environment.
- Reward Function, ğ‘Ÿ: ğ‘†, ğ´ â†’ ğ‘… s.t for each pair of state/action inputs (ğ‘ , ğ‘) it will output the reward r âˆˆ â„ an agent would obtain by taking an action ğ‘ from state ğ‘  in this environment.
- Transition Function, ğ‘¡ğ‘“: ğ‘†, ğ´, p â†’ ğ‘† s.t for each pair of state-action inputs (ğ‘ 3, ğ‘) it will output a state ğ‘ 4 an agent would transition into if the agent would take action ğ‘ from state ğ‘ 3 (i.e. if agent is in ğ‘ 3 = [0,1], and I take action ğ‘ = ğ‘ˆğ‘ƒ, then ğ‘¡ğ‘“(ğ‘ 3, ğ‘) = ğ‘ 4 = [0,2]).

The environment will be observable, non-deterministic, static, discrete and single-agent.

### 3.2. Q-learner
The Q-learner will be specified as a class (i.e. Q-learning.py). The Q-learnner will be built based on a environment. Once we have the set sets, actions, the deterministc and the non-deterministic transition functions and the reward functions the Q-learner will generate a matrix Q:|S|x|A| with the Q(s,a) value for each state-action pair. The Q-learner will have the following components:
- Environment, env: class ğ‘’, such that ğ‘’ will be the specific environment the agent is acting on.
- Q-learning function: Will return the matrix Q:|S|x|A| s.t. for each pair of state functions s,a, q[s][a] will return the Q(s,a) value which is equivalent to the maximum cumulative reward that can be achieved by performing action ğ‘ on state ğ‘ 

### 3.3. Intelligent Agent Specification
The intelligent agent will be specified as a class (i.e. i-agent.py). To build a complete agent we need the environment it will run on and the Q-learning matrix generated by the Q-learner for the specific environment. The agent will have the following components.
- State, s: the current state the agent finds himself in. t
- Environment, env: class ğ‘’, such that ğ‘’ will be the specific environment the agent is acting on.
- Knowledge Base: matrix Q s.t. for each pair of state-actions, ğ‘„[ğ‘ ][ğ‘] is equivalent to the ğ‘„(ğ‘ , ğ‘) obtained from a Q-learning algorithm.
- Transition Function, ğ‘¡ğ‘“: ğ‘†, ğ´ â†’ ğ‘† s.t for each pair of state-action inputs (ğ‘ 3, ğ‘) it will output a new state ğ‘ 4 an agent would transition to if the agent would take action ğ‘ from state ğ‘ 3. (i.e. if agent is in ğ‘ 3 = [0,1], and I take action ğ‘ = ğ‘ˆğ‘ƒ, then ğ‘¡ğ‘“(ğ‘ 3, ğ‘) = ğ‘ 4 = [0,2])
- Agent Function, ğ‘¡ğ‘“: ğ‘† â†’ ğ´ s.t for each input state ğ‘ , it will output the action ğ‘ that provides the maximum reward.
- Safety Boolean Function, ğ‘ ğ‘: ğ‘†, ğ´ â†’ {ğ‘‡ğ‘Ÿğ‘¢ğ‘’, ğ¹ğ‘ğ‘™ğ‘ ğ‘’} s.t. for each pair of state/action inputs (ğ‘ , ğ‘) it will output ğ‘‡ğ‘Ÿğ‘¢ğ‘’ if the state obtained by applying ğ‘ to ğ‘  is safe and it will return ğ¹ğ‘ğ‘™ğ‘ ğ‘’ if it is not safe.

### 3.4. PEAS
These are the specific settings our agent will be working in:
- Performance measure: Safety of agent and ability to reach a goal from an initial state sstart
- Environment: Grid world with an initial state, a goal state and craters.
- Actuators: move forwards, backwards, to the right and to the left.
- Sensors: Ability of agent to inspect its current state.

## 4. Algorithm
We will use the reinforcement learning algorithm Q-learning to train our agent to find the most efficient and safety path from itâ€™s initial state to the goal. In order to teach our agent the most safe and optimal solution our Q-learning algorithm will generate an optimal value ğ‘„(ğ‘ , ğ‘) for each state-action pair in the environment that will return the maximum cumulative reward that can be achieved by performing action ğ‘ on state ğ‘ , therefore we will be able to determine which action ğ‘ is the most optimal for each possible state ğ‘ . Once a user inputs the necessary data we will go through the following steps in order to obtain the safest/optimum path an agent should take from the initial state to the goal state:

1. Build the environment our agent will be acting on from input data
2. Run Q-learning algorithm to obtain Q(s,a) for all possible state-action pairs
3. Build Agent based on Q table and Environment
4. Given initial state provided on input calculate safest possible path.

Assumptions: We will have access to a set of states and actions, a reward matrix and transition matrix. Our algorithm will deal with a non-deterministic environment. For the test bed we will assume that given an action ğ‘, our agent will have a 95% chance of performing that action and a 5% chance of taking any other possible action. We will consider this non-deterministic actions when running our Q-learning algorithm and when building the path of our agent, this is further described in the next
section.
```
Input:
sF: initial state for our agent.
S: set of states of size n
A: set of actions of size m
R: set of possible rewards/punishments per state
T: transition matrix of size nxm s.t. T[s3][a] stores the state s4 that an agent would
arrive to if it were to perform action a on s3

Build Environment based on Input
# will consider non-deterministic environment when running Q-learning algorithm
Run Q-learning algorithm on environment and obtain Q(s,a) for each pair state-action.
Build Agent ag based on Input, Environment and Q-learning algorithm
While agent is not in goal state then i=0â€¦N
  Action_to_perform = ag.next_action()
  # next_action() is agâ€™s agent function based on each Q(s,a) obtained during Q learning
  algorithm
  Action_to_perform = Choose_randomly(Action_to_perform, other_actions, 0.95, 0.15)
  #safety
  If (ag.safe_state(Action_to_perform)) then action[i] = Action_to_perform
  Else then perform ag.next_action() until action obtained is safe
end while loop
Output grid map with action[] array of action to go from initial state to goal state
```

## 5. Safety

The grid map in which the planetary robot will be operating will have a certain number of crates, the safety concern will be to make sure the robot avoids falling into the crates while searching for itâ€™s goal. The crate will represent a dangerous situation by providing a negative reward (punishment) if the robot were to move in itâ€™s direction.
For other domains, where we do not have crater like in this test bed the algorithm will adapt to any environment as long as there is a negative reward value (punishment) to any state that challenges the safety of the agent.
Also, our environment will be non-deterministic. So in term of the Q-learning algorithm we will have to take into consideration the probability of taking a specific action (95%) and itâ€™s potential reward of taking a certain action plus the sum of products of the probability the agent will take another action (5% total) and the potential reward of said actions. In terms of building our path once we have performed the Q-learning algorithm, every action we take when building the safest path there is a 5% chance that our agent will perform an action other than the one itâ€™s supposed to take, therefore before taking any steps our agent will have to ensure (with itâ€™s built-in function) that the new state itâ€™s about to land on is safe, and if itâ€™s not safe it should recalculate a new action.
